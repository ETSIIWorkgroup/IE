{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "<!--<img src=\"images/Apache_Spark_logo.png\" width=\"30%\" align=\"center\">-->\n",
    "<img src=\"images/intro-logo-spark-spa.png\" width=\"30%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "En este notebook presentaremos los conceptos básicos de Spark y su uso desde Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "## Marco de trabajo\n",
    "\n",
    "#### Hasta ahora:\n",
    "Las tareas de \"data science\" y análisis de datos se han llevado a cabo “a pequeña escala”, en R/Python/MATLAB, etc.\n",
    "\n",
    "\n",
    "#### Hoy en día:\n",
    "Los conjuntos de datos ya no caben en memoria, así que...\n",
    "\n",
    "* Estos lenguajes/frameworks no nos permiten escalar. \n",
    "\n",
    "* Hay que reimplementarlo todo en algún otro lenguaje o sistema.\n",
    "\n",
    "\n",
    "#### Así que:\n",
    "\n",
    "* La industria se está moviendo hacia una Inteligencia Empresarial basada en la toma de decisiones orientada a los datos, que se apoya en enormes conjuntos de datos.\n",
    "\n",
    "* Principales retos: tratamiento de datos no estructurados, Big Data, datos en streaming, necesidad de HPC (*High Performance Computing*)\n",
    "\n",
    "* Principales herramientas: paradigma *MapReduce*, *Hadoop*... y, en los últimos años, *Spark*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es Spark?\n",
    "\n",
    "\n",
    "* Motor multi-lenguaje, aunque implementado en **Scala**, para la ejecución de procesos de Machine Learning y Data Science en máquinas de un sólo nodo o en clusters de forma *(casi)* transparente para el desarrollador.\n",
    "\n",
    "* Una de las herramientas más utilizadas en los últimos años en el ámbito del Big Data, desbancando a Hadoop o añadiéndole una capa de abstracción por encima.\n",
    "\n",
    "* La API de Spark guarda una relación casi de 1-a-1 con las colecciones más comunes de lenguajes de programación de alto nivel (sobre todo de *Scala* y *Java*)... pero ¡distribuidas!\n",
    "\n",
    "* Manejo de datos a través de estructuras de datos conocidas de otros lenguajes y librerías (¿os suenan *R*, *Pandas*,...?): tendremos **Dataframes**, pero... distribuidos o, mejor \"distribuibles\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es Scala?\n",
    "\n",
    "* Lenguaje Funcional Orientado a Objetos, basado en la JVM, publicado en 2003 (por aquél entonces Java estaba en su versión 4), diseñado, entre otros, por *Martin Odersky*, también diseñador del compilador GJ, que dio lugar a **javac**.\n",
    "\n",
    "* Comparte muchas similitudes a nivel sintáctico con Java.\n",
    "\n",
    "* A partir de su versión 6, Java incorporó numerosas ideas de Scala.\n",
    "\n",
    "* Scala simplifica **mucho** la sintaxis de Java, eliminando bastante *Syntactic Sugar* y con una inferencia de tipos bastante potente.\n",
    "\n",
    "* Mecanismo de **pattern matching** para implementar instrucciones tipo *switch* más sofisticadas y extensibles, sobre todo a nivel de clases.\n",
    "\n",
    "* Existen discusiones sobre si el futuro de Java será Scala o Kotlin, en términos de qué características serían deseables que incluyera Java en futuras versiones del lenguaje.\n",
    "\n",
    "* Al ser funcional, son conceptos muy importantes: la **inmutabilidad** de las colecciones sobre la mutabilidad, la **recursión** sobre la iteración...\n",
    "\n",
    "* Gracias a estas características, este lenguaje es muy apropiado para su uso en entornos **Big Data** y **Data Science**.\n",
    "\n",
    "* Tiene documentación y tutoriales online bastante completos.\n",
    "\n",
    "* Pero... **¡¡Es funcional!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "\n",
    "<br><br><br>\n",
    "<img src=\"images/spark_vs_hadoop.png\" width=\"30%\" align=\"left\">\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "\n",
    "\n",
    "* Hadoop es una implementación `open source` del MapReduce de Google.\n",
    "\n",
    "\n",
    "* Spark está implementado en Scala, un lenguaje funcional orientado a objetos, basado en la JVM.\n",
    "\n",
    "\n",
    "* Ambos ofrecen una API simple para operaciones map y reduce sobre conjuntos de datos distribuidos.\n",
    "\n",
    "\n",
    "* Tolerancia a fallos: entre cada operación map y reduce, se escriben datos intermedios para ser capaz de recuperarse de fallos.\n",
    "\n",
    "\n",
    "* La tolerancia a fallos de Spark es mucho más eficiente que en Hadoop porque:\n",
    "    - Mantiene todos los datos inmutables y en memoria\n",
    "    - Las operaciones son transformaciones funcionales\n",
    "    - Tolerancia a fallos: volver a aplicar las transformaciones a los datos originales\n",
    "\n",
    "\n",
    "* Spark es compatible con HDFS (Hadoop Distributed FileSystem)\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "# Conceptos principales en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "* Spark Session: una conexión a la API de Spark\n",
    "\n",
    "\n",
    "* Estructura Hardware:\n",
    "    - Cluster de master + workers\n",
    "    - Workflow: shuffling\n",
    "\n",
    "\n",
    "* Estructuras de datos lógicas:\n",
    "    - RDDs\n",
    "    - PairRDDs\n",
    "    \n",
    "    \n",
    "* Operaciones básicas:\n",
    "    - Transformaciones\n",
    "    - Acciones\n",
    "    \n",
    "    \n",
    "* Librerías interesantes:\n",
    "    - Spark SQL: DataFrames (y Datasets)\n",
    "    - Spark Streaming API\n",
    "    - MlLib\n",
    "    - GraphX\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text"
    ]
   },
   "source": [
    "# Spark Session\n",
    "\n",
    "### Conexión al cluster de Spark. \n",
    "\n",
    "Normalmente le \"hablaremos\" al nodo `máster` del cluster, y este le envía las tareas a los nodos `worker`.\n",
    "\n",
    "`SparkSession` es el objeto que usaremos para llevar a cabo las operaciones de configuración y entrada contra el cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"NOMBRE\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Las versiones anteriores de Spark usan <strong>SparkContext</strong> en lugar de <strong>SparkSession</strong></li>\n",
    "                <li><strong>SparkContext</strong> todavía se usa, pero es transparente al desarrollador</li>\n",
    "                <li>Se puede acceder a <strong>SparkContext</strong> a través de <strong>SparkSession</strong>: <strong>spark.sparkContext</strong></li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Hello World!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1,1001))\n",
    "rdd.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplo nos vale, pero en realidad ya tenemos una función para sumar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spark_structure.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "* El nodo 'master' distribuye los datos en bloques sobre los nodos 'worker', envía las tareas e integra los resultados de la computación.\n",
    "\n",
    "\n",
    "* Los nodos 'worker' reciben los fragmentos de datos y las tareas y llevan a cabo las transformaciones y acciones sobre sus bloques de datos.\n",
    "\n",
    "\n",
    "* Cada vez que nuestro proceso requiere todo el conjunto de datos para llevar a cabo una acción, el nodo 'master' recupera los bloques de datos de los 'workers', y reconstruye los datos en memoria.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important;text-align:left;\">\n",
    "            <ul>\n",
    "                <li>Cuando los datos viajan a través de la red, se denomina <em>shuffle</em> y es <strong>realmente costoso</strong></li>\n",
    "                <li>Debemos minimizar el número de veces que se requiere un <em>shuffle</em> en nuestra aplicación.</li>\n",
    "                <li>Pero vamos a tomarlo con calma por ahora: necesitamos saber más conceptos relacionados con Spark para estudiar con mayor profundidad este tema.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
