{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comenzando a programar con Python y Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructuras de datos lógicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RDDs\n",
    "\n",
    "* Resilient Distributed Dataset\n",
    "\n",
    "\n",
    "* Colecciones paralelas para la computación distribuida de programación funcional\n",
    "\n",
    "\n",
    "* Una colección de datos (tipados), que se puede distribuir fácilmente sobre los nodos 'worker', de manera que cada nodo se hace cargo de un trozo de todo el conjunto de datos a procesar.\n",
    "\n",
    "\n",
    "* Un RDD es una referencia lógica a un conjunto de datos que es fragmentado a través de muchos servidores en el cluster de Spark.\n",
    "\n",
    "\n",
    "* Los RDD son fragmentados y distribuidos sobre los nodos 'worker' en el cluster de Spark de forma automática (sin la intervención del programador). Ver la sección anterior sobre la estructura física de un cluster de Spark.\n",
    "\n",
    "\n",
    "* El esquema de fragmentación puede modificarse, pero por defecto Spark intenta minimizar el tráfico de la red entre los nodos cuando procesa los RDD. Por ejemplo: en un entorno local, hay normalmente una partición por cada nodo 'worker' (los cores de la CPU disponibles para Spark).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"NOMBRE\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1,1001))\n",
    "rdd.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones para RDDs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregaciones: conteo, suma..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones de Map-Reduce: map, reduce, filter, flatmap..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2 = rdd.map(lambda x: x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacar por pantalla los elementos del RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos del RDD: PythonRDD[37] at collect at <ipython-input-22-0fbf847bdd64>:1\n"
     ]
    }
   ],
   "source": [
    "print(\"Elementos del RDD: %s\" % rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ups! Error típico cuando empezamos a usar Spark: para aplicar una función de este tipo (lo explicaremos más adelante) primero debemos \"recopilar\" los datos en el *driver*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos del RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "print(\"Elementos del RDD: %s\" % rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otra opción es usar *take* para especificar cuántos elementos queremos \"recolectar\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elementos del RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "elementos del RDD: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40]\n"
     ]
    }
   ],
   "source": [
    "print(\"elementos del RDD: %s\" % rdd.take(20))\n",
    "print(\"elementos del RDD: %s\" % rdd_2.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "* Conceptualmente equivalente a una tabla SQL\n",
    "\n",
    "\n",
    "* Los DataFrame están compuestos por <strong>filas</strong> (sin tipo)\n",
    "\n",
    "\n",
    "* Perdimos la flexibilidad de los RDD (los tipos) y las funciones definidos por el programador, contra un conjunto de tipos predefinidos (dependientes del lenguaje que se utilice para interactuar con Spark) y funciones relacionales (<em>SELECT, COUNT, WHERE...</em>)\n",
    "\n",
    "\n",
    "* Por otro lado, obtenemos enormes <strong>optimizaciones</strong> en términos de eficiencia en tiempo gracias a estas fuertes restricciones.\n",
    "\n",
    "\n",
    "* Catalyst es el componente Spark a cargo de las optimizaciones de esos métodos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** Existe otra estructura de datos en Spark, los Datasets, pero no están disponibles en lenguajes con tipificación débil, como Python o R, y sí en lenguajes de tipificación fuerte, como Java o Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: Leyendo de un fichero Json con Pyspark\n",
    "\n",
    "\n",
    "Fichero Json de entrada:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[...]\n",
    "\n",
    "{\n",
    "    \"idTweet\":\"915831976929714177\",\n",
    "    \n",
    "    \"text\":\"RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… \",\n",
    "    \n",
    "    \"date\":\"Thu Oct 05 08:52:13 CEST 2017\",\n",
    "    \n",
    "    \"authorId\":\"2885455811\",\n",
    "    \n",
    "    \"idOriginal\":\"915523419281739776\"\n",
    "}\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones en Spark para leer el fichero y construir un Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authorId: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- idOriginal: string (nullable = true)\n",
      " |-- idTweet: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "|  authorId|                date|        idOriginal|           idTweet|                text|\n",
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "|2885455811|Thu Oct 05 08:52:...|915523419281739776|915831976929714177|RT @Societatcc: A...|\n",
      "|   2099361|Thu Oct 05 08:52:...|                  |915831940745441280|Yo ya he escogido...|\n",
      "| 799792832|Thu Oct 05 08:52:...|915830958443687936|915831968301973504|RT @pedroveraOyP:...|\n",
      "| 105157939|Thu Oct 05 08:52:...|915523419281739776|915831985582612480|RT @Societatcc: A...|\n",
      "| 124248712|Thu Oct 05 08:52:...|915830958443687936|915832004658286593|RT @pedroveraOyP:...|\n",
      "| 110117638|Thu Oct 05 08:48:...|                  |915830958443687936|#AmicsAmigos no p...|\n",
      "| 150587014|Thu Oct 05 08:52:...|915830945785237504|915832008936509440|RT @gsemprunmdg: ...|\n",
      "| 273360453|Thu Oct 05 08:52:...|915808416639143936|915832057288433664|RT @carmouna: Si ...|\n",
      "| 184865048|Thu Oct 05 07:18:...|                  |915808416639143936|Si no lo arreglan...|\n",
      "| 142775869|Thu Oct 05 09:10:...|                  |915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# volcar archivo JSON a un dataframe\n",
    "df = spark.read.json(\"test.txt\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a jugar un poco con nuestro nuevo Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'text'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @Societatcc: A...|\n",
      "|Yo ya he escogido...|\n",
      "|RT @pedroveraOyP:...|\n",
      "|RT @Societatcc: A...|\n",
      "|RT @pedroveraOyP:...|\n",
      "|#AmicsAmigos no p...|\n",
      "|RT @gsemprunmdg: ...|\n",
      "|RT @carmouna: Si ...|\n",
      "|Si no lo arreglan...|\n",
      "|La elegancia del ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = df.select(df.text)\n",
    "tweets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtra aquellos tuits que tengan cualquier hashtag\n",
    "\n",
    "Con lo que sabemos hasta ahora, lo mejor es pasar un Dataframe a RDD y a partir de ahí actuar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', '#AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr', 'RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… ', 'Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz']\n"
     ]
    }
   ],
   "source": [
    "# RDD a partir de un Dataframe\n",
    "testRDD = tweets.rdd.flatMap(list)\n",
    "\n",
    "hashtagTweets = testRDD.filter(lambda t: \"#\" in t)\n",
    "print(hashtagTweets.count())\n",
    "print(hashtagTweets.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta: ¿Cómo podemos obtener los hashtags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recupera los tuits que han sido retuiteados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… ', 'RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\n¿Com… ', 'RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @gsemprunmdg: el desbroce    x      Davila\\n\\n#FelizJueves\\n#AmicsAmigos\\n#LaCafeteraPARLEM\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr', 'RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… ']\n"
     ]
    }
   ],
   "source": [
    "hashtagTweets = testRDD.filter(lambda t: t.startswith(\"RT\"))\n",
    "print(hashtagTweets.count())\n",
    "print(hashtagTweets.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairRDDs\n",
    "\n",
    "\n",
    "* Intuición: versión paralela y distribuida de un Map\n",
    "\n",
    "\n",
    "* Un RDD que contiene tuplas de (clave, valor)\n",
    "\n",
    "\n",
    "* Muy útil porque los Map son una de las abstracciones de datos más utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso de uso de PairRDDs: Contando palabras en un RDD\n",
    "\n",
    "\n",
    "1.- Primero: vamos a dividir el contenido del RDD en palabras: usando <strong>flatMap</strong>\n",
    "\n",
    "2.- Después, crea un PairRDD con: (Palabra, 1): usando <strong>map</strong>\n",
    "\n",
    "3.- Finalmente, agrupa cada par en función de su primer componente (la palabra) y suma los segundos componentes (ocurrencias de las palabras): usando la función <strong>reduceByKey</strong> de los PairRDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras leídas: 70\n",
      "[('RT', 6), ('@Societatcc:', 2), ('Ayúdanos', 2), ('a', 8), ('difundir,', 2), ('necesitamos', 2), ('llegar', 2), ('todos', 4), ('los', 4), ('rincones,', 2), ('no', 7), ('tenemos', 4), ('TV3', 2), ('pero...', 2), ('¡¡os', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Mecanismo habitual para contar elementos mapeando un RDD a un PairRDD\n",
    "countWords = testRDD.flatMap(lambda line: line.split(\" \")).map(lambda w: (w, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(\"Palabras leídas: %d\" % countWords.count())\n",
    "\n",
    "# Imprimiendo\n",
    "print(countWords.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a pensarlo dos veces...\n",
    "\n",
    "\n",
    "* El primer paso va desde un RDD[String] a un RDD[String]: flatMap divide cada <em>Tweet</em> en una Collection[<em>palabras</em>], y luego las aplana, obteniendo un RDD[<em>palabras</em>].\n",
    "\n",
    "\n",
    "* El segundo paso va de un RDD[String], donde cada String es una palabra, a un RDD[(String, Int)], que es un PairRDD[(String, Int)].\n",
    "\n",
    "* Finalmente, <strong>reduceByKey</strong> agrupa todas las tuplas con la misma palabra, sumando sus valores, produciendo un PairRDD[(String, Int)] que representa un RDD[<em>(palabra, ocurrenciasDePalabra)</em>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Pero, realmente... es un poco tedioso, ¿no es cierto?\n",
    "\n",
    "\n",
    "   1.- No son buenos para procesar datos estructurados o semi-estructurados: \n",
    "    \n",
    "   - En el ejemplo, intentamos leer un fichero **estructurado**, en formato Json\n",
    "    \n",
    "   - Así que perdimos toda esa <strong>información preciada</strong> (campos, valores, etc.) transformándolo en una colección (resiliente y distribuible) de strings.\n",
    "    \n",
    "   - Y luego usamos los mismos <strong>split-get-replace</strong> viejos y aburridos de la clase String para extraer las partes interesantes del string.\n",
    "\n",
    "\n",
    "   2.- El shuffling puede convertirse en el cuello de botella de nuestra aplicación y a veces no es fácil de evitar.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>En relación al <strong>data shuffle</strong>, en la siguiente sección estudiaremos las operaciones básicas en Spark (<em>Transformaciones</em> y <em>Acciones</em>), sus efectos y la manera en que se gestionan en el cluster de Spark.</li>\n",
    "                <li>Respecto al procesado de <strong>información estructurada y semi-estructurada</strong>: Spark ofrece una manera mucho mejor de lidiar con este tipo de datos a través de la librería <em>Spark SQL</em> y sus estructuras de datos relacionales: <em>DataFrames</em> y <em>Datasets</em>. Las estudiaremos más adelante.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<img src=\"images/sparkSQL.png\" width=\"30%\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Características de Spark SQL\n",
    "\n",
    "\n",
    "* Librería de Spark que integra la sintaxis basada en SQL para realizar operaciones en datos distribuidos.\n",
    "\n",
    "\n",
    "* Define estructuras de datos para facilitar la implementación de operaciones relacionales (select, group-by, order-by, max, min, average, count, etc.): DataFrames y Datasets.\n",
    "\n",
    "\n",
    "* Estas estructuras de datos integran optimizaciones de rendimiento del álgebra relacional de SQL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jugando con DataFrames y Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los DataFrames pueden usarse casi como una base de datos relacional SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "| authorId|                date|idOriginal|           idTweet|                text|\n",
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "|  2099361|Thu Oct 05 08:52:...|          |915831940745441280|Yo ya he escogido...|\n",
      "|110117638|Thu Oct 05 08:48:...|          |915830958443687936|#AmicsAmigos no p...|\n",
      "|184865048|Thu Oct 05 07:18:...|          |915808416639143936|Si no lo arreglan...|\n",
      "|142775869|Thu Oct 05 09:10:...|          |915836526789046273|La elegancia del ...|\n",
      "+---------+--------------------+----------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrar el DataFrame como una vista SQL temporal\n",
    "df.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "# Seleccionar tweets que NO son retweets\n",
    "originalTweetsQueryDF = spark.sql(\"SELECT * FROM tweets WHERE idOriginal LIKE ''\")\n",
    "\n",
    "originalTweetsQueryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El equivalente, usando funciones Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|idOriginal|                date| authorId|           idTweet|                text|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "|          |Thu Oct 05 08:52:...|  2099361|915831940745441280|Yo ya he escogido...|\n",
      "|          |Thu Oct 05 08:48:...|110117638|915830958443687936|#AmicsAmigos no p...|\n",
      "|          |Thu Oct 05 07:18:...|184865048|915808416639143936|Si no lo arreglan...|\n",
      "|          |Thu Oct 05 09:10:...|142775869|915836526789046273|La elegancia del ...|\n",
      "+----------+--------------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar tweets que NO son retweets\n",
    "df.select(df.idOriginal, df.date, df.authorId, df.idTweet, df.text).where(\"idOriginal LIKE''\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregaciones:\n",
    "\n",
    "* Una de las tareas más comunes con las bases de datos relacionales es agrupar y/o agregar atributos con ciertas condiciones para realizarle algunas acciones al resultado, como contar, sumar, calcular la media, etc.\n",
    "\n",
    "\n",
    "* Spark SQL proporciona la función <strong>groupBy</strong>, que devuelve un <em>RelationalGroupedDataset</em>\n",
    "\n",
    "\n",
    "* Este tipo tiene una serie de funciones de agregación relacional: sum, count, avg, max, min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ejemplo de agrupación:\n",
    "grouped = df.groupBy(df.idOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idOriginal: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contando por idOriginal:\n",
    "groupedCount = grouped.count()\n",
    "groupedCount.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        idOriginal|count|\n",
      "+------------------+-----+\n",
      "|                  |    4|\n",
      "|915830958443687936|    2|\n",
      "|915523419281739776|    2|\n",
      "|915830945785237504|    1|\n",
      "|915808416639143936|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenando los resultados\n",
    "groupedCount.orderBy(groupedCount[\"count\"].desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(count)|\n",
      "+----------+\n",
      "|       2.0|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|         4|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average, max, min...\n",
    "from pyspark.sql import functions as F\n",
    "groupedCount.agg(F.avg(groupedCount[\"count\"])).show()\n",
    "groupedCount.agg(F.max(groupedCount[\"count\"])).show()\n",
    "groupedCount.agg(F.min(groupedCount[\"count\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con esto último que hemos aprendido, ¿cómo sacamos los tweets que tengan algún hashtag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<img src=\"images/spark_transformations_actions.png\" width=\"40%\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Operaciones Básicas de Spark: Transformaciones y Acciones\n",
    "\n",
    "\n",
    "\n",
    "Los RDD de Apache Spark soportan dos tipos de operaciones: Transformaciones y Acciones.\n",
    "\n",
    "\n",
    "\n",
    "## Transformaciones\n",
    "\n",
    "\n",
    "* Son funciones que producen nuevos RDD a partir de los ya existentes. Eejemplos: map(), filter().\n",
    "\n",
    "\n",
    "* Dado que los RDD de entrada no pueden modificarse (son inmutables por naturaleza), cada vez que aplicamos una transformación se crean nuevos RDD.\n",
    "\n",
    "\n",
    "* Las transformaciones se evalúan con evaluación \"perezosa\", lo que significa que no se ejecutan de inmediato. Una transformación se ejecuta efectivamente cuando llamamos a una acción.\n",
    "\n",
    "\n",
    "* Por lo tanto, aplicar una (cantidad de) transformaciones no produce ningún efecto inmediato. En cambio, se crea un linaje de RDD, que va del RDD original (que invoca la primera transformación) a los RDD finales (resultado de todas las transformaciones). El linaje de RDD, representado por un <strong>DAG</strong> (Directed Acyclic Graph o Grafo acíclico dirigido), es un plan de ejecución lógica de todas las transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos de transformaciones y del DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[107] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[99] at mapPartitions at PythonRDD.scala:133 []\n",
      " |  ShuffledRDD[98] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(1) PairwiseRDD[97] at reduceByKey at <ipython-input-62-5b85601e5432>:2 []\n",
      "    |  PythonRDD[96] at reduceByKey at <ipython-input-62-5b85601e5432>:2 []\n",
      "    |  MapPartitionsRDD[65] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "    |  MapPartitionsRDD[64] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "    |  SQLExecutionRDD[63] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "    |  MapPartitionsRDD[62] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "    |  FileScanRDD[61] at javaToPython at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(countWords.toDebugString().decode('UTF-8'))            # imprimir el plan de ejecución (DAG) de las transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de transformaciones:\n",
    "\n",
    "\n",
    "\n",
    "* Transformaciones estrechas ('narrow'): no implican una mezcla de datos. Se pueden calcular por cada nodo 'worker' con sus propias particiones de datos.\n",
    "     - Ejemplos: map, filter, flatMap, union, sample...\n",
    "\n",
    "\n",
    "\n",
    "* Transformaciones amplias ('wide'): la lógica de procesamiento depende de los datos de múltiples particiones, por lo que es necesario combinar los datos para reunirlos en un solo lugar.\n",
    "     - Ejemplos: distinct, join, reduceByKey, groupByKey...\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/notepad.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Spark implementa un mecanismo para optimizar el plan de ejecución de las transformaciones con el fin de minimizar la combinación de datos ('data shuffling')</li>\n",
    "                <li>Recuerda que las transformaciones son <strong>lazy</strong>: no se ejecutan cuando se declaran</li>\n",
    "                <li>Una forma de realizar un conjunto de transformaciones es aplicar una acción al RDD de salida</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/warning.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>DAG es el mecanismo que permite que Spark sea tolerante a fallos, <strong>sin</strong> tener que escribir datos en el disco como una copia de seguridad</li>\n",
    "                <li>Spark se recupera de las fallos volviendo a calcular las particiones perdidas, siguiendo el <strong>DAG</strong></li>\n",
    "                <li>Es realmente <strong>rápido</strong> recuperar datos de transformaciones <strong>narrow</strong>, pero <strong>lento</strong> hacerlo de transformaciones <strong>wide</strong></li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Acciones:\n",
    "\n",
    "\n",
    "\n",
    "* Son operaciones Spark sobre RDD que producen valores que no son RDD.\n",
    "\n",
    "\n",
    "* Los resultados de las acciones se almacenan en los nodos 'master' o en el sistema de almacenamiento externo. Por lo tanto, una acción es una de las maneras de enviar datos desde los nodos 'worker' al 'master'.\n",
    "\n",
    "\n",
    "* Pone el modo 'lazy' de los RDD en movimiento, lo que significa que una acción provoca la ejecución de las transformaciones asociadas en el RDD.\n",
    "\n",
    "* Ejemplos: count, collect, first, take..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repasemos el ejemplo de RDD desde un archivo de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalRDD = spark.sparkContext.textFile(\"test.txt\")           # Leer fichero de texto plano\n",
    "\n",
    "firstTransformation = originalRDD.map(lambda row: row.split(\"\\\",\"))\n",
    "\n",
    "secondTransformation = firstTransformation.map(lambda row: row[1].replace(\"\\\"text\\\":\\\"\", \"\"))\n",
    "\n",
    "thirdTransformation = secondTransformation.filter(lambda text: \"@\" in text)\n",
    "\n",
    "fourthTransformation = secondTransformation.flatMap(lambda text: text.split(\" \"))\n",
    "\n",
    "fifthTransformation = fourthTransformation.filter(lambda word: word.startswith(\"#\"))\n",
    "\n",
    "sixthTransformation = fifthTransformation.map(lambda x: x.lower()).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/question.jpg\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>¿Qué hemos hecho hasta el momento?</li>\n",
    "                <li>¿Cuál es el contenido de cada RDD?</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\\\n¿Com… ', 'RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\\\n¿Com… ', 'RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @gsemprunmdg: el desbroce    x      Davila\\\\n\\\\n#FelizJueves\\\\n#AmicsAmigos\\\\n#LaCafeteraPARLEM\\\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr', 'RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… ', 'Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz']\n",
      "\n",
      "['#AmicsAmigos', '#ranciofacts', '#AmicsAmigos', '#ranciofacts', '#AmicsAmigos', '#ranciofacts', '#amicsamigos', '#amicsamigos']\n",
      "\n",
      "['#amicsamigos', '#ranciofacts']\n"
     ]
    }
   ],
   "source": [
    "print(thirdTransformation.take(10))         # Transformación a computar: 3, 2 y 1\n",
    "print()\n",
    "print(fifthTransformation.take(10))         # Transformación a computar: 5, 4, 2 y 1\n",
    "print()\n",
    "print(sixthTransformation.take(10))         # Transformación a computar: 6, 5, 4, 2 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-collapse: collapse; border: none !important; width: 100%;\">\n",
    "    <tr style=\"border:none !important;\">\n",
    "        <td style=\"border:none !important; width: 60px;\">\n",
    "<img src=\"icons/optimizar.png\" align=\"left\" width=\"50px\"> \n",
    "        </td>\n",
    "        <td style=\"border:none !important; text-align:left\">\n",
    "            <ul>\n",
    "                <li>Recuerda que las transformaciones se evaluan mediante evaluación perezosa, así que... </li>\n",
    "                <li>Fíjate en que las transformaciones 2 y 1 se evalúan ¡¡tres veces!!</li>\n",
    "                <li>Spark proporciona un mecanismo para ayudar a los programadores a evitar esta situación: <strong>caching</strong>. Vamos a reescribir nuestro código:</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "originalRDD2 = spark.sparkContext.textFile(\"test.txt\")         # leer el fichero de texto plano\n",
    "\n",
    "firstT = originalRDD2.map(lambda row: row.split(\"\\\",\"))\n",
    "\n",
    "secondT = firstT.map(lambda row: row[1].replace(\"\\\"text\\\":\\\"\", \"\")).cache()    # ¡¡Guardar el resultado en la caché!!\n",
    "\n",
    "thirdT = secondT.filter(lambda text: \"@\" in text)\n",
    "\n",
    "fourthT = secondT.flatMap(lambda text: text.split(\" \"))\n",
    "\n",
    "fifthT = fourthT.filter(lambda word: word.startswith(\"#\"))\n",
    "\n",
    "sixthT = fifthT.map(lambda x: x.lower()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\\\n¿Com… ', 'RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @Societatcc: Ayúdanos a difundir, necesitamos llegar a todos los rincones, no tenemos TV3 pero... ¡¡os tenemos a vosotros!!\\\\n¿Com… ', 'RT @pedroveraOyP: #AmicsAmigos no pelearsen que es muy #ranciofacts https://t.co/mjMhHQfHuB', 'RT @gsemprunmdg: el desbroce    x      Davila\\\\n\\\\n#FelizJueves\\\\n#AmicsAmigos\\\\n#LaCafeteraPARLEM\\\\n#DíaMundialDeLosDocentes https://t.co/trDDTvrjgr', 'RT @carmouna: Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… ', 'Si no lo arreglan los que mandan, lo haremos todos nosotros. Juntos. Envía tu canción a #amicsamigos @radio3_rne… https://t.co/ayQQEgCvVz']\n",
      "\n",
      "['#AmicsAmigos', '#ranciofacts', '#AmicsAmigos', '#ranciofacts', '#AmicsAmigos', '#ranciofacts', '#amicsamigos', '#amicsamigos']\n",
      "\n",
      "['#amicsamigos', '#ranciofacts']\n"
     ]
    }
   ],
   "source": [
    "print(thirdT.take(10))         # Transformación a computar: 3, 2 y 1, y guarda en la caché la transformación 2\n",
    "print()\n",
    "print(fifthT.take(10))         # Transformación a computar: 5 and 4 sobre la ya ya evaluada y guardada 2\n",
    "print()\n",
    "print(sixthT.take(10))         # Transformación a computar: 6, 5, 4 sobre la ya ya evaluada y guardada 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
